{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9938be31",
   "metadata": {},
   "source": [
    "# 1. Install and Setup Dependinces for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85650f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stable Baseline library-->Python library that make it easier to get up and running with RL\n",
    "#It is originally based off openAI baseline package But has additional features that make it easier to get started with RL\n",
    "# Rl baseline3 zoo provides a collection of pre-trained agents, scripts of training , evaluation agents, tuning hyperparameter \n",
    "# plotting results and recording videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9a2a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                  *  C:\\Users\\HP\\anaconda3\n",
      "YOLOv7                   C:\\Users\\HP\\anaconda3\\envs\\YOLOv7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5c9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bf0613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: gym==0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.5.1)\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.13.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from stable-baselines3[extra]) (2.11.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.64.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.4.1)\n",
      "Requirement already satisfied: ale-py==0.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.7.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (9.0.1)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.10.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.27.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\hp\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.19.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (4.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2021.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e89ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control] in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gym[classic_control]) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gym[classic_control]) (2.0.0)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gym[classic_control]) (2.0.7)\n"
     ]
    }
   ],
   "source": [
    "# I m using CartPole-env0 which belong to \"classical_control env\"\n",
    "# To working with these environmnet we should install Unique Dependencies\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0b237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.7)\n"
     ]
    }
   ],
   "source": [
    "# These library help us to visualzed our environment\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da944eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyOpenGL in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyOpenGL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0571ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97db65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e1efc",
   "metadata": {},
   "source": [
    "# 2.Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cce08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we looked at the different components that required for the RL\n",
    "# so environmnet for RL are one of most important component\n",
    "# Environment may be Simulated and Real environmnet\n",
    "# Simulated Environment: gives you the ability to trail and train a model in a safe AND cost effective manner\n",
    "#after training Rl alogirthms on simulated environmnet deploy into production(real environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287834a",
   "metadata": {},
   "source": [
    "###### Open AI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1645be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openAIGym provides you with an easy way to build environment for training RL agents\n",
    "# openAIGym environment is represented by something are called openAIGym spaces\n",
    "# This gives you a kicking off point rather than having to write all of the code yourself\n",
    "#The following are the OpenAIGym space:\n",
    "#1: Box: n-dimensional tensor, range of values \n",
    "#2: Discrete -- set of item\n",
    "#3: Tuple-- tuple of other spaces e.g box or discrete\n",
    "#4:Dict: dictionary of spaces e.g box or discrete\n",
    "#5: MultiBinary: one-hot encoded binary values\n",
    "#6:MultiDiscrete: Multi discrete values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3209b4d",
   "metadata": {},
   "source": [
    "##### Load Environment from OpenAIGym(provide alot of environment) and we also build our own custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb5e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v0'\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7669310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9c560c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenGL.GL import glPushMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bf0e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:16.0\n",
      "Episode:2 Score:10.0\n",
      "Episode:3 Score:55.0\n",
      "Episode:4 Score:43.0\n",
      "Episode:5 Score:12.0\n"
     ]
    }
   ],
   "source": [
    "# I m going to write a code to test the environment\n",
    "# env.reset()----> with help of these we can reset the enivornment and obtain the initial observation\n",
    "# env.render()----> with help of these we can visualize the environmnet \n",
    "# env.step()----> with help of these we can apply an action to the environment\n",
    "# env.close()----> with help of these we can close down the render frame\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()    # it will get inital set of observation\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1441e693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01786559, 0.0485202 , 0.03926175, 0.03468588], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca8551c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11808083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478d4a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.018836  ,  0.24305777,  0.03995547, -0.2453556 ], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0dcf79",
   "metadata": {},
   "source": [
    "# Understand the Environment\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65d9749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole env --> having to part\n",
    "# 1: Action space\n",
    "# 2: observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bce7be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "688205f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 means to push cart into the right\n",
    "# 0 means to push cart into the left\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "415c73bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e88e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1592190e+00,  2.6987214e+37, -2.9874784e-01,  3.3277435e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e4c5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     | Num | Observation           | Min                 | Max               |\n",
    "#     |-----|-----------------------|---------------------|-------------------|\n",
    "#     | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "#     | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "#     | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "#     | 3   | Pole Angular Velocity | -Inf                | Inf               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e981837",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "123f4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# First go to main directory and create \"Training and logs\" directory\n",
    "log_path = os.path.join(\"Training\",\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af6d0453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\logs'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "970df80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5d1fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88efd84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\PPO_5\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 580  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 756          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077058026 |\n",
      "|    clip_fraction        | 0.0833       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.686       |\n",
      "|    explained_variance   | 0.00398      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.83         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 53.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 801        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01038492 |\n",
      "|    clip_fraction        | 0.0791     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.669     |\n",
      "|    explained_variance   | 0.0618     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.5       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 31.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 837         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009705525 |\n",
      "|    clip_fraction        | 0.0898      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 56          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007745941 |\n",
      "|    clip_fraction        | 0.0615      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 63.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 907         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010306411 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 62.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 926          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035287251 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.596       |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.4         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00423     |\n",
      "|    value_loss           | 47.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 924         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003999768 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 39.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 937         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005504459 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.31        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    value_loss           | 33.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 955          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070480346 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.954        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.51         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00702     |\n",
      "|    value_loss           | 14.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27a7a65e430>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7b5a7",
   "metadata": {},
   "source": [
    "###### Save and reload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "512b5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join(\"Training\", 'save_models', 'PPO_Model_CartPole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c27b0e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\save_models\\\\PPO_Model_CartPole'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02e3d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d79320e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73dcfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb68b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the model that we saved\n",
    "model = PPO.load(PPO_path,env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4577cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27a0e5f0d00>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4295c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\PPO_6\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1979 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27a0e5f0d00>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e00ab",
   "metadata": {},
   "source": [
    "###### Testing and Evalution of Train RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659c32b",
   "metadata": {},
   "source": [
    "###### Evalution of our Train(PPO) RL Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8456a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glPushMatrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:124\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 current_lengths[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 124\u001b[0m         \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m mean_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(episode_rewards)\n\u001b[0;32m    127\u001b[0m std_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(episode_rewards)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:98\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03mGym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m:param mode: The rendering type.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:229\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcarttrans\u001b[38;5;241m.\u001b[39mset_translation(cartx, carty)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoletrans\u001b[38;5;241m.\u001b[39mset_rotation(\u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:126\u001b[0m, in \u001b[0;36mViewer.render\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mdispatch_events()\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m geom \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeoms:\n\u001b[0;32m    128\u001b[0m     geom\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:232\u001b[0m, in \u001b[0;36mTransform.enable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 232\u001b[0m     \u001b[43mglPushMatrix\u001b[49m()\n\u001b[0;32m    233\u001b[0m     glTranslatef(\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    235\u001b[0m     )  \u001b[38;5;66;03m# translate to GL loc ppint\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     glRotatef(RAD2DEG \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glPushMatrix' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e5c1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3ece6",
   "metadata": {},
   "source": [
    "###### Test our Train RL algorithms(PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dd0d89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[200.]\n",
      "Episode:2 Score:[182.]\n",
      "Episode:3 Score:[182.]\n",
      "Episode:4 Score:[200.]\n",
      "Episode:5 Score:[200.]\n"
     ]
    }
   ],
   "source": [
    "# I m going to write a code to test the environment\n",
    "# env.reset()----> with help of these we can reset the enivornment and obtain the initial observation\n",
    "# env.render()----> with help of these we can visualize the environmnet \n",
    "# env.step()----> with help of these we can apply an action to the environment\n",
    "# env.close()----> with help of these we can close down the render frame\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()    # it will get inital set of observation\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action,_ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "# env.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cbdf75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3734d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "009dcad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00154419,  0.03000139, -0.04114381, -0.04363587]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ffc552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64), None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f454af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00094416,  0.22568843, -0.04201652, -0.3490109 ]],\n",
       "       dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([False]),\n",
       " [{}])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fae3ee",
   "metadata": {},
   "source": [
    "###### Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ea89fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"!\" ---> are called megic command -->with the help of these run command line from the our notebooke\n",
    "training_log_path = os.path.join(log_path, 'PPO_2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9343dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\logs\\\\PPO_2'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f671f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m tensorboard.main --logdir=."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e581725",
   "metadata": {},
   "source": [
    "###### Adding a Callback to the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3724128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you are taining a huge model or model that are going to take long time to train\n",
    "# and you want to stop the training of the Rl-algo to reach a certain reward threshold then we\n",
    "# used callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0922e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c536055",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8773b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'save_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "557f58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(env,\n",
    "                            callback_on_new_best=stop_callback,\n",
    "                            eval_freq=1000,\n",
    "                            best_model_save_path=save_path,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7df2090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "defef58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=70.20 +/- 14.33\n",
      "Episode length: 70.20 +/- 14.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=58.60 +/- 18.42\n",
      "Episode length: 58.60 +/- 18.42\n",
      "Eval num_timesteps=3000, episode_reward=150.60 +/- 6.65\n",
      "Episode length: 150.60 +/- 6.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=148.40 +/- 9.60\n",
      "Episode length: 148.40 +/- 9.60\n",
      "Eval num_timesteps=5000, episode_reward=189.20 +/- 21.60\n",
      "Episode length: 189.20 +/- 21.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=171.20 +/- 33.88\n",
      "Episode length: 171.20 +/- 33.88\n",
      "Eval num_timesteps=7000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 200.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27a15ab7880>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c85fa6",
   "metadata": {},
   "source": [
    "###### Changing policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06442648",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c242f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:457: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d3fc758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\PPO_9\n",
      "Eval num_timesteps=952, episode_reward=80.00 +/- 18.47\n",
      "Episode length: 80.00 +/- 18.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 80       |\n",
      "|    mean_reward     | 80       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1952, episode_reward=114.40 +/- 44.39\n",
      "Episode length: 114.40 +/- 44.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 114      |\n",
      "|    mean_reward     | 114      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1952     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1174 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2952, episode_reward=192.40 +/- 15.20\n",
      "Episode length: 192.40 +/- 15.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 192         |\n",
      "|    mean_reward          | 192         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2952        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016700549 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.33        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 23.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3952, episode_reward=191.40 +/- 17.20\n",
      "Episode length: 191.40 +/- 17.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 191      |\n",
      "|    mean_reward     | 191      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3952     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 741  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4952        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013604459 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.599      |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5952     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 643  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | 200        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6952       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00824659 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.571     |\n",
      "|    explained_variance   | 0.555      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.91       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    value_loss           | 40.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7952     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 596  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8952        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008535115 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.87        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 25.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9952     |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 580   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10952       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012366488 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.992       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 9.74        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11952    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 571   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12952        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072148107 |\n",
      "|    clip_fraction        | 0.0961       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 0.793        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.03         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00433     |\n",
      "|    value_loss           | 5.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13952    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 563   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 14336 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14952        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069336104 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.419        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00532     |\n",
      "|    value_loss           | 4.37         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15952    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 550   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16952        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047576725 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.512       |\n",
      "|    explained_variance   | 0.0368       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.794        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.000841     |\n",
      "|    value_loss           | 8.2          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17952    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 548   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 18432 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18952        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038385203 |\n",
      "|    clip_fraction        | 0.0714       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.501       |\n",
      "|    explained_variance   | -0.0117      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.364        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 2.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19952, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19952    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 545   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27a158b7700>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092c825",
   "metadata": {},
   "source": [
    "###### Use an Alternative Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cfabca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47b413c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ec9a480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x27a18c85880>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f8ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
